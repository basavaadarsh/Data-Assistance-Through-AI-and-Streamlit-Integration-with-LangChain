import os
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import streamlit as st
from transformers import AutoModelForCausalLM, AutoTokenizer
from cryptography.fernet import Fernet
import io
import base64
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
from sklearn.svm import SVR
from sklearn.metrics import mean_squared_error

# Define your Hugging Face API key here
apikey = "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"

# Initialize the tokenizer and model
model_name = "gpt2"  # Replace with the model you need
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForCausalLM.from_pretrained(model_name)

# Initialize encryption key and cipher (For real usage, load this from a secure location)
key = Fernet.generate_key()
cipher_suite = Fernet(key)

def generate_text(prompt):
    inputs = tokenizer(prompt, return_tensors="pt")
    outputs = model.generate(inputs["input_ids"], max_length=150)
    return tokenizer.decode(outputs[0], skip_special_tokens=True)

# Encryption and Decryption Functions
def encrypt_data(data):
    return cipher_suite.encrypt(data.encode())

def decrypt_data(encrypted_data):
    try:
        decrypted_data = cipher_suite.decrypt(encrypted_data).decode()
        return decrypted_data
    except Exception as e:
        st.error(f"Decryption failed: {e}")
        raise

def plt_to_base64(fig):
    """Convert matplotlib figure to base64 for embedding in HTML"""
    buf = io.BytesIO()
    fig.savefig(buf, format='png')
    buf.seek(0)
    return base64.b64encode(buf.read()).decode('utf-8')

# Title
st.title('AI Assistant for Data Science ü§ñ')

# Welcoming message
st.write("Hello, üëã I am your AI Assistant, and I am here to help you with your data science projects.")

# Explanation sidebar
with st.sidebar:
    st.write('*Your Data Science Adventure Begins with a CSV File.*')
    st.caption('''**Upload a CSV file to begin. I will analyze the data, provide insights, generate visualizations, 
    and suggest appropriate machine learning models to tackle your problem. Let‚Äôs dive into your data science journey!**
    ''')
    st.divider()
    st.caption("<p style ='text-align:center'> made with ‚ù§Ô∏è by Kare</p>", unsafe_allow_html=True)

# Initialize the key in session state
if 'clicked' not in st.session_state:
    st.session_state.clicked = {1: False}

# Function to update the value in session state
def clicked(button):
    st.session_state.clicked[button] = True

st.button("Let's get started", on_click=clicked, args=[1])

if st.session_state.clicked[1]:
    user_csv = st.file_uploader("Upload your file here", type="csv")
    if user_csv is not None:
        # Encrypt the file for demonstration (you can test decryption with a sample encrypted file)
        file_data = user_csv.read()
        encrypted_file = encrypt_data(file_data.decode(errors='ignore'))

        # Notify user about encryption
        st.write("Your data has been successfully encrypted.")

        try:
            decrypted_file = decrypt_data(encrypted_file)
            df = pd.read_csv(io.StringIO(decrypted_file), low_memory=False)
        except Exception as e:
            st.error(f"An error occurred during decryption: {e}")
            st.stop()

        # Function sidebar
        @st.cache_data
        def steps_eda():
            steps_eda = generate_text('What are the steps of EDA?')
            return steps_eda

        # Functions main
        @st.cache_data
        def function_agent():
            st.write("**Data Overview**")
            st.write("The first rows of your dataset look like this:")
            st.write(df.head())

            st.write("**Data Cleaning**")
            st.write("Missing values in each column:")
            null_values = df.isnull().sum()
            st.write(null_values[null_values > 0])

            st.write("Duplicate values in the dataset:")
            duplicates = df[df.duplicated()]
            if not duplicates.empty:
                st.write(duplicates)
            else:
                st.write("No duplicate values found.")

            st.write("**Data Summarisation**")
            st.write(df.describe())

            st.write("**Visualizations**")

            # Heatmap of missing values
            fig, ax = plt.subplots()
            sns.heatmap(df.isnull(), cbar=False, cmap='viridis', ax=ax)
            ax.set_title('Heatmap of Missing Values')
            st.pyplot(fig)

            # Histograms of numeric features
            num_features = df.select_dtypes(include=['number']).columns
            if len(num_features) > 0:
                fig, ax = plt.subplots()
                df[num_features].hist(ax=ax, bins=30, figsize=(10, 7))
                ax.set_title('Histograms of Numeric Features')
                st.pyplot(fig)
            else:
                st.write("No numeric features available for histograms.")

            # Pairplot for visualizing relationships between numeric features
            if len(num_features) > 1:
                pairplot_fig = sns.pairplot(df[num_features])
                pairplot_fig.fig.suptitle('Pairplot of Numeric Features', y=1.02)
                st.pyplot(pairplot_fig.fig)

            # Suggest ML models based on data types
            st.markdown(
            """
            <p style='font-size: 25px;'><strong>Suggested Machine Learning Models</strong></p>
            """,
            unsafe_allow_html=True
            )

            if 'object' in df.dtypes.values:
                st.write("Since your dataset contains categorical data, consider using models like Decision Trees, Random Forests, or Gradient Boosting.")
            else:
                st.write("Since your dataset is numeric, consider using models like Linear Regression, Support Vector Machines, or Neural Networks.")

        @st.cache_data
        def function_question_variable(selected_variable):
            # Provide insights on the selected variable
            st.write(f"**Analyzing Variable: {selected_variable}**")

            # Summary statistics
            summary_statistics = df[selected_variable].describe()
            st.write("**Summary Statistics**")
            st.write(summary_statistics)

            # Check for normality
            fig, ax = plt.subplots()
            sns.histplot(df[selected_variable], kde=True, ax=ax)
            ax.set_title(f'Distribution of {selected_variable}')
            st.pyplot(fig)

            # Assess outliers
            lower_bound = df[selected_variable].quantile(0.25) - 1.5 * (df[selected_variable].quantile(0.75) - df[selected_variable].quantile(0.25))
            upper_bound = df[selected_variable].quantile(0.75) + 1.5 * (df[selected_variable].quantile(0.75) - df[selected_variable].quantile(0.25))
            outlier_count = ((df[selected_variable] < lower_bound) | (df[selected_variable] > upper_bound)).sum()
            st.write(f"**Number of Outliers for {selected_variable}:** {outlier_count}")

            # Missing values
            missing_values = df[selected_variable].isnull().sum()
            st.write(f"**Missing Values in {selected_variable}:** {missing_values}")

            # Further analysis (trends, seasonality, etc.)
            trends_analysis = generate_text(f"Analyze trends for {selected_variable}")
            st.write(trends_analysis)

        # Main
        st.header('Exploratory Data Analysis')
        st.subheader('General information about the data')
        function_agent()

        # Variable Study
        st.header('Variable Study')
        user_question_variable = st.selectbox("Select a variable for study", [''] + df.columns.tolist())
        if user_question_variable:
            function_question_variable(user_question_variable)

        # Dataframe Study
        st.header('DataFrame Study')
        user_question_dataframe = st.text_area("Ask a question about the DataFrame")
        if user_question_dataframe:
            dataframe_info = generate_text(user_question_dataframe)
            st.write(dataframe_info)

       # Predictive Modeling
        st.header('Predictive Modeling')
        st.write("**Select Feature Columns and Target Column for Prediction**")
        feature_cols = st.multiselect("Select Feature Columns", df.columns.tolist())
        target_col = st.selectbox("Select Target Column", df.columns.tolist())
        num_years = st.number_input("Enter number of years for prediction", min_value=1, value=1)
        model_choice = st.selectbox("Select Machine Learning Model", ["Linear Regression", "Random Forest", "Gradient Boosting", "Support Vector Regression"])

        # Check for missing values in target variable
        if target_col:
            missing_target = df[target_col].isnull().sum()
            if missing_target > 0:
                st.warning(f"The target column '{target_col}' contains {missing_target} missing values. Please handle them before training the model.")
            else:
                # Dynamic Hyperparameter Adjustment
                st.subheader("Advanced Model Options")
                if model_choice == "Random Forest":
                    n_estimators = st.slider("Number of Estimators", 10, 100, 10)
                    max_depth = st.slider("Max Depth", 2, 10, 5)
                elif model_choice == "Gradient Boosting":
                    learning_rate = st.slider("Learning Rate", 0.01, 0.3, 0.1)
                    n_estimators = st.slider("Number of Estimators", 10, 100, 10)
                elif model_choice == "Support Vector Regression":
                    C = st.slider("C (Regularization)", 0.01, 10.0, 1.0)
                    epsilon = st.slider("Epsilon", 0.01, 1.0, 0.1)

                if st.button("Train Advanced Model"):
                    if target_col:
                        X = df[feature_cols]
                        y = df[target_col]

                        # Drop rows with NaN values in feature or target variables
                        if X.isnull().values.any() or y.isnull().values.any():
                            st.warning("Data contains NaN values. Please clean the data.")
                        else:
                            X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

                            if model_choice == "Linear Regression":
                                model = LinearRegression()
                            elif model_choice == "Random Forest":
                                model = RandomForestRegressor(n_estimators=n_estimators, max_depth=max_depth)
                            elif model_choice == "Gradient Boosting":
                                model = GradientBoostingRegressor(learning_rate=learning_rate, n_estimators=n_estimators)
                            elif model_choice == "Support Vector Regression":
                                model = SVR(C=C, epsilon=epsilon)

                            model.fit(X_train, y_train)
                            predictions = model.predict(X_test)
                            mse = mean_squared_error(y_test, predictions)
                            rmse = mse ** 0.5
                            st.write(f"**Model Accuracy (RMSE):** {rmse:.2f}")

                            # Predict future state
                            future_prediction = model.predict(X)  # Replace with actual future data
                            st.write(f"**The market is expected to be high in the future after {num_years} years:** {future_prediction.mean():.2f}")


        # LangChain Integration
        st.header("AI-Driven Data Insights with LangChain")
        st.write("Ask any data-related question, and the AI will provide insights.")

        user_query = st.text_area("Your Question", "Explain trends in the data...")
        if user_query:
            response = generate_text(user_query)  # Mock response
            st.write(f"AI Response: {response}")

        st.write("Thank you for using the AI Assistant for Data Science! üöÄ")
